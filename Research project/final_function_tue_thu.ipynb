{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b23a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.sandbox.stats.runs import runstest_1samp\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import  gaussian_kde\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c38e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>weekday</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>VOL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1984-09-07</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0.10122</td>\n",
       "      <td>0.10122</td>\n",
       "      <td>97236149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984-09-10</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0.10122</td>\n",
       "      <td>0.10062</td>\n",
       "      <td>75471114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1984-09-11</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0.10153</td>\n",
       "      <td>0.10246</td>\n",
       "      <td>177965367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1984-09-12</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0.10246</td>\n",
       "      <td>0.09938</td>\n",
       "      <td>155467926.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1984-09-13</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>242135546.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE    weekday     OPEN    CLOSE          VOL\n",
       "0  1984-09-07     Friday  0.10122  0.10122   97236149.0\n",
       "1  1984-09-10     Monday  0.10122  0.10062   75471114.0\n",
       "2  1984-09-11    Tuesday  0.10153  0.10246  177965367.0\n",
       "3  1984-09-12  Wednesday  0.10246  0.09938  155467926.0\n",
       "4  1984-09-13   Thursday  0.10490  0.10490  242135546.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple = pd.read_csv(r\"D:\\data\\notebooks\\week-4\\cleaned_apple.csv\")\n",
    "\n",
    "apple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca18fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def full_strategy_pipeline(params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Weekly trading pipeline:\n",
    "      - Builds weekly Tue→Thu dataset with thu/tue multipliers.\n",
    "      - Rolling train/validate/test Decision Tree with threshold tuning.\n",
    "      - Computes confusion counts, precision, chattiness, correctness.\n",
    "      - Runs test for randomness of correctness.\n",
    "      - Uniformity (chi-square) across time with a chosen bin size.\n",
    "      - Historical Monte Carlo using empirical TP/FP thu/tue multipliers over subsets.\n",
    "      - Future Monte Carlo from last subset.\n",
    "      - Baseline comparisons: always trade, random trader, alternate-week trader, weighted-coin trader.\n",
    "      - Returns a report-card dictionary and pretty-prints it.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # --------------------------- INPUTS --------------------------\n",
    "    # ============================================================\n",
    "\n",
    "    df = params[\"df\"]\n",
    "\n",
    "    # Rolling / model config\n",
    "    VALID_WEEKS       = params[\"VALID_WEEKS\"]\n",
    "    depth_grid        = params[\"depth_grid\"]\n",
    "    leaf_grid         = params[\"leaf_grid\"]\n",
    "    thresholds_tested = params[\"thresholds_tested\"]\n",
    "    FIXED             = params[\"FIXED\"]\n",
    "\n",
    "    # Scoring weights\n",
    "    alpha_p = params[\"alpha_p\"]\n",
    "    alpha_c = params[\"alpha_c\"]\n",
    "    p_min   = params[\"p_min\"]\n",
    "    c_min   = params[\"c_min\"]\n",
    "\n",
    "    # Monte Carlo settings (second half)\n",
    "    n_subsets      = params[\"n_subsets\"]\n",
    "    n_trajectories = params[\"n_trajectories\"]\n",
    "    n_weeks        = params[\"n_weeks\"]\n",
    "    initial_bank   = params[\"initial_bank\"]\n",
    "    upper_thresh   = params[\"upper_thresh\"]\n",
    "    lower_thresh   = params[\"lower_thresh\"]\n",
    "    rng_seed       = params[\"rng_seed\"]\n",
    "\n",
    "    # Uniformity\n",
    "    uniformity_binsize = params[\"uniformity_binsize\"]\n",
    "\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    # ============================================================\n",
    "    # ---------- PART I: CLEANING + WEEKLY DATASET ---------------\n",
    "    # ============================================================\n",
    "\n",
    "    df = df.sort_values(\"DATE\").reset_index(drop=True)\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])   # let pandas infer the format\n",
    "\n",
    "    # Normalization – your original style\n",
    "    df[\"normalized_close\"] = (\n",
    "        (df[\"CLOSE\"] - df[\"CLOSE\"].expanding().mean().shift(1)) /\n",
    "        df[\"CLOSE\"].expanding().std(ddof=0).shift(1)\n",
    "    )\n",
    "    df[\"normalized_open\"] = (\n",
    "        (df[\"OPEN\"] - df[\"OPEN\"].expanding().mean().shift(1)) /\n",
    "        df[\"OPEN\"].expanding().std(ddof=0).shift(1)\n",
    "    )\n",
    "\n",
    "    df[\"weekday\"] = df[\"DATE\"].dt.weekday\n",
    "    df[\"week\"]    = df[\"DATE\"].dt.to_period(\"W-SUN\")\n",
    "\n",
    "    # Tue and Thu opens – keep the names as-is\n",
    "    tue_open = (\n",
    "        df.loc[df[\"weekday\"] == 1]\n",
    "          .groupby(\"week\")[\"OPEN\"]\n",
    "          .first()\n",
    "          .rename(\"tue_open\")\n",
    "    )\n",
    "    thu_open = (\n",
    "        df.loc[df[\"weekday\"] == 3]\n",
    "          .groupby(\"week\")[\"OPEN\"]\n",
    "          .first()\n",
    "          .rename(\"thu_open\")\n",
    "    )\n",
    "\n",
    "    weekly = pd.concat([tue_open, thu_open], axis=1)\n",
    "\n",
    "    # tue → thu multiplier\n",
    "    weekly[\"thu/tue\"] = weekly[\"thu_open\"] / weekly[\"tue_open\"]\n",
    "\n",
    "    # keep your net% logic as well\n",
    "    weekly[\"net%\"]      = (weekly[\"thu/tue\"] - 1.0) * 100.0\n",
    "    weekly[\"week_type\"] = (weekly[\"thu/tue\"] > 1.0).astype(int)\n",
    "\n",
    "    # Normalised features for Tue and previous Thu/Fri\n",
    "    norm_tue_open = (\n",
    "        df.loc[df[\"weekday\"] == 1]\n",
    "          .set_index(\"week\")[\"normalized_open\"]\n",
    "          .rename(\"Norm_Tue_Open\")\n",
    "    )\n",
    "    norm_prev_thu_open = (\n",
    "        df.loc[df[\"weekday\"] == 3]\n",
    "          .set_index(\"week\")[\"normalized_open\"]\n",
    "          .rename(\"Norm_PrevThu_Open\")\n",
    "          .shift(1)\n",
    "    )\n",
    "    norm_prev_fri_open = (\n",
    "        df.loc[df[\"weekday\"] == 4]\n",
    "          .set_index(\"week\")[\"normalized_open\"]\n",
    "          .rename(\"Norm_PrevFri_Open\")\n",
    "          .shift(1)\n",
    "    )\n",
    "\n",
    "    weekly_full_norm = (\n",
    "        weekly.copy()\n",
    "              .join(norm_tue_open, how=\"left\")\n",
    "              .join(norm_prev_thu_open, how=\"left\")\n",
    "              .join(norm_prev_fri_open, how=\"left\")\n",
    "              .dropna()\n",
    "    )\n",
    "\n",
    "    features = [\"Norm_PrevThu_Open\", \"Norm_PrevFri_Open\", \"Norm_Tue_Open\"]\n",
    "    target   = \"week_type\"\n",
    "\n",
    "    # ============================================================\n",
    "    # ---------- PART II: ROLLING TRAIN-VAL-TEST -----------------\n",
    "    # ============================================================\n",
    "\n",
    "    def precision(tp, fp):\n",
    "        denom = tp + fp\n",
    "        return tp / denom if denom > 0 else 0.0\n",
    "\n",
    "    def chattiness(tp, fp, fn):\n",
    "        denom = tp + fn\n",
    "        return (tp + fp) / denom if denom > 0 else 0.0\n",
    "\n",
    "    def model_score(tp, fp, fn):\n",
    "        P = precision(tp, fp)\n",
    "        C = chattiness(tp, fp, fn)\n",
    "        s = np.exp(alpha_p * (P - p_min) + alpha_c * (C - c_min))\n",
    "        return 0.0 if np.isnan(s) or np.isinf(s) else float(s)\n",
    "\n",
    "    TP = TN = FP = FN = 0\n",
    "    weekly_best = []\n",
    "\n",
    "    for t in tqdm(range(VALID_WEEKS + 1, len(weekly_full_norm)), desc=\"Rolling simulation\"):\n",
    "        val_start = max(0, t - VALID_WEEKS)\n",
    "        training   = weekly_full_norm.iloc[:val_start]\n",
    "        validation = weekly_full_norm.iloc[val_start:t]\n",
    "        test       = weekly_full_norm.iloc[[t]]\n",
    "\n",
    "        # need both classes in training\n",
    "        if len(training[target].unique()) < 2:\n",
    "            continue\n",
    "\n",
    "        train_X, train_y = training[features], training[target]\n",
    "        val_X, val_y     = validation[features], validation[target]\n",
    "        test_X, test_y   = test[features], test[target]\n",
    "\n",
    "        best_score  = -np.inf\n",
    "        best_params = None\n",
    "        best_model  = None\n",
    "\n",
    "        for depth, leaf in product(depth_grid, leaf_grid):\n",
    "            model = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=leaf, **FIXED)\n",
    "            model.fit(train_X, train_y)\n",
    "            probs_val = model.predict_proba(val_X)[:, 1]\n",
    "\n",
    "            for thr in thresholds_tested:\n",
    "                preds_val = (probs_val > thr).astype(int)\n",
    "                tp = ((preds_val == 1) & (val_y == 1)).sum()\n",
    "                fp = ((preds_val == 1) & (val_y == 0)).sum()\n",
    "                fn = ((preds_val == 0) & (val_y == 1)).sum()\n",
    "                sc = model_score(tp, fp, fn)\n",
    "                if sc > best_score:\n",
    "                    best_score  = sc\n",
    "                    best_params = (depth, leaf, thr)\n",
    "                    best_model  = model\n",
    "\n",
    "        best_depth, best_leaf, best_thr = best_params\n",
    "        p_hat = best_model.predict_proba(test_X)[0, 1]\n",
    "        pred  = int(p_hat > best_thr)\n",
    "        true  = int(test_y.iloc[0])\n",
    "\n",
    "        if   pred == 1 and true == 1:\n",
    "            TP += 1; outcome = \"TP\"\n",
    "        elif pred == 0 and true == 0:\n",
    "            TN += 1; outcome = \"TN\"\n",
    "        elif pred == 1 and true == 0:\n",
    "            FP += 1; outcome = \"FP\"\n",
    "        else:\n",
    "            FN += 1; outcome = \"FN\"\n",
    "\n",
    "        thu_tue_val = float(test[\"thu/tue\"].iloc[0])\n",
    "\n",
    "        weekly_best.append(dict(\n",
    "            Week=t,\n",
    "            Best_Depth=best_depth,\n",
    "            Best_Leaf=best_leaf,\n",
    "            Best_Threshold=float(best_thr),\n",
    "            Best_Score=best_score,\n",
    "            True_Label=true,\n",
    "            Pred_Label=pred,\n",
    "            Outcome=outcome,\n",
    "            thu_tue=thu_tue_val\n",
    "        ))\n",
    "\n",
    "    df_final = pd.DataFrame(weekly_best)\n",
    "    if df_final.empty:\n",
    "        raise ValueError(\"df_final is empty; check VALID_WEEKS and data length.\")\n",
    "\n",
    "    # ============================================================\n",
    "    # ---------------- BASIC METRICS + TESTS ---------------------\n",
    "    # ============================================================\n",
    "\n",
    "    total = TP + TN + FP + FN\n",
    "    correctness_rate = (TP + TN) / total if total > 0 else 0.0\n",
    "    prec_overall     = precision(TP, FP)\n",
    "    chat_overall     = chattiness(TP, FP, FN)\n",
    "\n",
    "    denom_pos = TP + FP\n",
    "    pct_fp_when_pred_pos = FP / denom_pos if denom_pos > 0 else 0.0\n",
    "\n",
    "    # Runs test on correctness\n",
    "    df_final[\"correct\"] = (df_final[\"True_Label\"] == df_final[\"Pred_Label\"]).astype(int)\n",
    "    z_runs, p_runs = runstest_1samp(df_final[\"correct\"], correction=False)\n",
    "    randomness_test = {\n",
    "        \"H0\": \"Correctness sequence is random over time.\",\n",
    "        \"z_statistic\": float(z_runs),\n",
    "        \"p_value\": float(p_runs),\n",
    "    }\n",
    "\n",
    "    # Uniformity test with chosen bin size\n",
    "    df_final[\"chunk\"] = df_final.index // uniformity_binsize\n",
    "    tab = pd.crosstab(df_final[\"chunk\"], df_final[\"correct\"])\n",
    "    chi2, p_chi, dof, _ = chi2_contingency(tab)\n",
    "    uniformity_test = {\n",
    "        \"H0\": f\"Correctness rate is uniform across time (binsize={uniformity_binsize} weeks).\",\n",
    "        \"chi2\": float(chi2),\n",
    "        \"p_value\": float(p_chi),\n",
    "        \"degrees_of_freedom\": int(dof),\n",
    "        \"binsize\": int(uniformity_binsize),\n",
    "    }\n",
    "\n",
    "    # Longest TP/FP streaks\n",
    "    def longest_streak(seq, label):\n",
    "        best = 0\n",
    "        cur  = 0\n",
    "        for x in seq:\n",
    "            if x == label:\n",
    "                cur += 1\n",
    "                if cur > best:\n",
    "                    best = cur\n",
    "            else:\n",
    "                cur = 0\n",
    "        return best\n",
    "\n",
    "    longest_tp = longest_streak(df_final[\"Outcome\"], \"TP\")\n",
    "    longest_fp = longest_streak(df_final[\"Outcome\"], \"FP\")\n",
    "\n",
    "    # ============================================================\n",
    "    # ----------------- PART III: HISTORICAL MC ------------------\n",
    "    # ============================================================\n",
    "\n",
    "    outcomes_arr = np.array([\"TP\", \"FP\", \"FN\", \"TN\"])\n",
    "\n",
    "    def build_samplers(tp_vals, fp_vals):\n",
    "        tp_sorted = np.sort(tp_vals)\n",
    "        fp_sorted = np.sort(fp_vals)\n",
    "\n",
    "        n_tp = len(tp_sorted)\n",
    "        n_fp = len(fp_sorted)\n",
    "\n",
    "        tp_cdf = np.arange(1, n_tp + 1) / n_tp\n",
    "        fp_cdf = np.arange(1, n_fp + 1) / n_fp\n",
    "\n",
    "        def sample_tp(size=1):\n",
    "            u   = rng.uniform(0.0, 1.0, size)\n",
    "            idx = np.searchsorted(tp_cdf, u, side=\"right\")\n",
    "            idx = np.clip(idx, 0, n_tp - 1)\n",
    "            return tp_sorted[idx]\n",
    "\n",
    "        def sample_fp(size=1):\n",
    "            u   = rng.uniform(0.0, 1.0, size)\n",
    "            idx = np.searchsorted(fp_cdf, u, side=\"right\")\n",
    "            idx = np.clip(idx, 0, n_fp - 1)\n",
    "            return fp_sorted[idx]\n",
    "\n",
    "        return sample_tp, sample_fp\n",
    "\n",
    "    def run_mc_block(p, sample_tp, sample_fp):\n",
    "        cdf = np.cumsum(p)\n",
    "        final_banks = np.empty(n_trajectories)\n",
    "\n",
    "        def step_mult(outcome):\n",
    "            if outcome == \"TP\":\n",
    "                return float(sample_tp(1)[0])\n",
    "            if outcome == \"FP\":\n",
    "                return float(sample_fp(1)[0])\n",
    "            return 1.0  # FN, TN: no trade\n",
    "\n",
    "        for i in range(n_trajectories):\n",
    "            bank = initial_bank\n",
    "            for _ in range(n_weeks):\n",
    "                r = rng.random()\n",
    "                idx = np.searchsorted(cdf, r, side=\"right\")\n",
    "                outcome = outcomes_arr[idx]\n",
    "                bank *= step_mult(outcome)\n",
    "                if bank >= upper_thresh or bank <= lower_thresh:\n",
    "                    break\n",
    "            final_banks[i] = bank\n",
    "\n",
    "        return final_banks\n",
    "\n",
    "    def run_actual(sub: pd.DataFrame) -> float:\n",
    "        bank = initial_bank\n",
    "        for _, row in sub.iterrows():\n",
    "            if row[\"Outcome\"] in (\"TP\", \"FP\"):\n",
    "                bank *= row[\"thu_tue\"]\n",
    "            if bank >= upper_thresh or bank <= lower_thresh:\n",
    "                break\n",
    "        return bank\n",
    "\n",
    "    # Split df_final into approx equal subsets\n",
    "    raw_subsets = np.array_split(df_final, n_subsets)\n",
    "    subsets = [s for s in raw_subsets if len(s) > 0]\n",
    "\n",
    "    all_sims = []\n",
    "    actual_balances = []\n",
    "    null_percentiles = []\n",
    "    valid_mc_subsets = []\n",
    "\n",
    "    for sub in subsets:\n",
    "        tp_vals = sub.loc[sub[\"Outcome\"] == \"TP\", \"thu_tue\"].values\n",
    "        fp_vals = sub.loc[sub[\"Outcome\"] == \"FP\", \"thu_tue\"].values\n",
    "\n",
    "        if len(tp_vals) < 2 or len(fp_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        sample_tp, sample_fp = build_samplers(tp_vals, fp_vals)\n",
    "        counts = sub[\"Outcome\"].value_counts(normalize=True)\n",
    "        p = np.array([counts.get(k, 0.0) for k in outcomes_arr])\n",
    "\n",
    "        sim = run_mc_block(p, sample_tp, sample_fp)\n",
    "        all_sims.append(sim)\n",
    "\n",
    "        actual = run_actual(sub)\n",
    "        actual_balances.append(actual)\n",
    "\n",
    "        null_pct = float(np.mean(sim <= actual))\n",
    "        null_percentiles.append(null_pct)\n",
    "\n",
    "        valid_mc_subsets.append(sub)\n",
    "\n",
    "    if not all_sims:\n",
    "        raise ValueError(\"No valid subsets for Monte Carlo (need TP and FP in some windows).\")\n",
    "\n",
    "    sim_all = np.concatenate(all_sims)\n",
    "    actual_balances = np.array(actual_balances)\n",
    "\n",
    "    simulated_mean_balance   = float(np.mean(sim_all))\n",
    "    simulated_median_balance = float(np.median(sim_all))\n",
    "\n",
    "    ks_distance, ks_p_value = ks_2samp(actual_balances, sim_all)\n",
    "    average_null_percentile = float(np.mean(null_percentiles))\n",
    "\n",
    "    # ============================================================\n",
    "    # --------------------- PART IV: FUTURE MC -------------------\n",
    "    # ============================================================\n",
    "\n",
    "    last = valid_mc_subsets[-1]\n",
    "    tp_vals_last = last.loc[last[\"Outcome\"] == \"TP\", \"thu_tue\"].values\n",
    "    fp_vals_last = last.loc[last[\"Outcome\"] == \"FP\", \"thu_tue\"].values\n",
    "\n",
    "    sample_tp_last, sample_fp_last = build_samplers(tp_vals_last, fp_vals_last)\n",
    "    counts_last = last[\"Outcome\"].value_counts(normalize=True)\n",
    "    p_last = np.array([counts_last.get(k, 0.0) for k in outcomes_arr])\n",
    "\n",
    "    fut = run_mc_block(p_last, sample_tp_last, sample_fp_last)\n",
    "\n",
    "    future_mean_balance   = float(np.mean(fut))\n",
    "    future_median_balance = float(np.median(fut))\n",
    "\n",
    "    prob_success   = float(np.mean(fut >= upper_thresh))\n",
    "    prob_failure   = float(np.mean(fut <= lower_thresh))\n",
    "    prob_uncertain = float(1.0 - prob_success - prob_failure)\n",
    "\n",
    "    # ============================================================\n",
    "    # --------------------- INTERNAL METRICS ---------------------\n",
    "    # ============================================================\n",
    "\n",
    "    TP_ret = df_final.loc[df_final[\"Outcome\"] == \"TP\", \"thu_tue\"].values\n",
    "    FP_ret = df_final.loc[df_final[\"Outcome\"] == \"FP\", \"thu_tue\"].values\n",
    "\n",
    "    denom_trades = len(TP_ret) + len(FP_ret)\n",
    "    precision_on_trades = float(len(TP_ret) / denom_trades) if denom_trades > 0 else 0.0\n",
    "    trade_frequency     = float(denom_trades / len(df_final))\n",
    "\n",
    "    mistake_asymmetry = float(np.mean(TP_ret) - np.mean(FP_ret)) if len(TP_ret) and len(FP_ret) else np.nan\n",
    "\n",
    "    gains  = df_final.loc[df_final[\"thu_tue\"] > 1.0, \"thu_tue\"].values\n",
    "    losses = df_final.loc[df_final[\"thu_tue\"] < 1.0, \"thu_tue\"].values\n",
    "\n",
    "    if len(gains) > 0 and len(losses) > 0:\n",
    "        macro_return_ratio = float(np.mean(gains) / abs(np.mean(losses)))\n",
    "        micro_pairs = min(len(gains), len(losses))\n",
    "        micro_return_ratio = float(np.mean(gains[:micro_pairs] / losses[:micro_pairs]))\n",
    "        return_ratio_gap   = float(macro_return_ratio - micro_return_ratio)\n",
    "    else:\n",
    "        macro_return_ratio = np.nan\n",
    "        micro_return_ratio = np.nan\n",
    "        return_ratio_gap   = np.nan\n",
    "\n",
    "    # ============================================================\n",
    "    # -------------------- BASELINE COMPARISONS ------------------\n",
    "    # ============================================================\n",
    "\n",
    "    ratio_always   = []\n",
    "    ratio_random   = []\n",
    "    ratio_alt      = []\n",
    "    ratio_weighted = []\n",
    "\n",
    "    for sub in subsets:\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "\n",
    "        model_bal = run_actual(sub)\n",
    "\n",
    "        # Always trade\n",
    "        b = initial_bank\n",
    "        for r in sub[\"thu_tue\"]:\n",
    "            b *= r\n",
    "            if b >= upper_thresh or b <= lower_thresh:\n",
    "                break\n",
    "        ratio_always.append(model_bal / b if b != 0 else np.nan)\n",
    "\n",
    "        # Random trader with same chattiness\n",
    "        trade_prob = len(sub.loc[sub[\"Outcome\"].isin([\"TP\", \"FP\"])]) / len(sub)\n",
    "        b = initial_bank\n",
    "        for r in sub[\"thu_tue\"]:\n",
    "            if rng.random() < trade_prob:\n",
    "                b *= r\n",
    "            if b >= upper_thresh or b <= lower_thresh:\n",
    "                break\n",
    "        ratio_random.append(model_bal / b if b != 0 else np.nan)\n",
    "\n",
    "        # Alternate-week trader\n",
    "        b = initial_bank\n",
    "        for i, r in enumerate(sub[\"thu_tue\"]):\n",
    "            if i % 2 == 0:\n",
    "                b *= r\n",
    "            if b >= upper_thresh or b <= lower_thresh:\n",
    "                break\n",
    "        ratio_alt.append(model_bal / b if b != 0 else np.nan)\n",
    "\n",
    "        # Weighted-coin trader (trade prob = fraction of >1 weeks)\n",
    "        good_rate = float(np.mean(sub[\"thu_tue\"] > 1.0))\n",
    "        b = initial_bank\n",
    "        for r in sub[\"thu_tue\"]:\n",
    "            if rng.random() < good_rate:\n",
    "                b *= r\n",
    "            if b >= upper_thresh or b <= lower_thresh:\n",
    "                break\n",
    "        ratio_weighted.append(model_bal / b if b != 0 else np.nan)\n",
    "\n",
    "    # ============================================================\n",
    "    # ------------------------ FINAL REPORT ----------------------\n",
    "    # ============================================================\n",
    "\n",
    "    report = {\n",
    "        \"historical_mc\": {\n",
    "            \"simulated_mean_balance\": simulated_mean_balance,\n",
    "            \"simulated_median_balance\": simulated_median_balance,\n",
    "            \"ks_distance\": float(ks_distance),\n",
    "            \"ks_p_value\": float(ks_p_value),\n",
    "            \"average_null_percentile\": average_null_percentile,\n",
    "        },\n",
    "        \"future_mc\": {\n",
    "            \"future_mean_balance\": future_mean_balance,\n",
    "            \"future_median_balance\": future_median_balance,\n",
    "            \"prob_success\": prob_success,\n",
    "            \"prob_failure\": prob_failure,\n",
    "            \"prob_uncertain\": prob_uncertain,\n",
    "        },\n",
    "        \"internal_metrics\": {\n",
    "            \"precision_overall\": float(prec_overall),\n",
    "            \"chattiness_overall\": float(chat_overall),\n",
    "            \"correctness_rate\": float(correctness_rate),\n",
    "            \"precision_on_trades\": precision_on_trades,\n",
    "            \"trade_frequency\": trade_frequency,\n",
    "            \"mistake_asymmetry_TP_minus_FP\": mistake_asymmetry,\n",
    "            \"macro_return_ratio\": macro_return_ratio,\n",
    "            \"micro_return_ratio\": micro_return_ratio,\n",
    "            \"return_ratio_gap\": return_ratio_gap,\n",
    "            \"longest_TP_streak\": int(longest_tp),\n",
    "            \"longest_FP_streak\": int(longest_fp),\n",
    "            \"%FP_when_predicted_positive\": float(pct_fp_when_pred_pos),\n",
    "        },\n",
    "        \"baseline_comparison\": {\n",
    "            \"ratio_vs_always_trade\": float(np.nanmean(ratio_always)),\n",
    "            \"ratio_vs_random_trader\": float(np.nanmean(ratio_random)),\n",
    "            \"ratio_vs_alternate_week_trader\": float(np.nanmean(ratio_alt)),\n",
    "            \"ratio_vs_weighted_coin_trader\": float(np.nanmean(ratio_weighted)),\n",
    "        },\n",
    "        \"uniformity_test\": uniformity_test,\n",
    "        \"randomness_test\": randomness_test,\n",
    "    }\n",
    "\n",
    "    print(\"\\n===== MODEL REPORT CARD =====\")\n",
    "    pprint(report)\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af546f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling simulation: 100%|██████████| 1909/1909 [53:40<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MODEL REPORT CARD =====\n",
      "{'baseline_comparison': {'ratio_vs_alternate_week_trader': 1.188748656120587,\n",
      "                         'ratio_vs_always_trade': 1.1551936421319584,\n",
      "                         'ratio_vs_random_trader': 1.1987025526240644,\n",
      "                         'ratio_vs_weighted_coin_trader': 1.1314085477320683},\n",
      " 'future_mc': {'future_mean_balance': 130.23050665682405,\n",
      "               'future_median_balance': 128.22582303621516,\n",
      "               'prob_failure': 0.00039,\n",
      "               'prob_success': 0.01272,\n",
      "               'prob_uncertain': 0.98689},\n",
      " 'historical_mc': {'average_null_percentile': 0.5226894444444444,\n",
      "                   'ks_distance': 0.09601944444444446,\n",
      "                   'ks_p_value': 0.9905969629687478,\n",
      "                   'simulated_mean_balance': 128.24714802704327,\n",
      "                   'simulated_median_balance': 121.1748874333227},\n",
      " 'internal_metrics': {'%FP_when_predicted_positive': 0.4537987679671458,\n",
      "                      'chattiness_overall': 0.9938775510204082,\n",
      "                      'correctness_rate': 0.5332983744100682,\n",
      "                      'longest_FP_streak': 8,\n",
      "                      'longest_TP_streak': 9,\n",
      "                      'macro_return_ratio': 1.0574821549461133,\n",
      "                      'micro_return_ratio': 1.0594461540759437,\n",
      "                      'mistake_asymmetry_TP_minus_FP': 0.051015287881198024,\n",
      "                      'precision_on_trades': 0.5462012320328542,\n",
      "                      'precision_overall': 0.5462012320328542,\n",
      "                      'return_ratio_gap': -0.0019639991298303627,\n",
      "                      'trade_frequency': 0.5107498689040377},\n",
      " 'randomness_test': {'H0': 'Correctness sequence is random over time.',\n",
      "                     'p_value': 0.046467381868778274,\n",
      "                     'z_statistic': -1.991122859601576},\n",
      " 'uniformity_test': {'H0': 'Correctness rate is uniform across time '\n",
      "                           '(binsize=104 weeks).',\n",
      "                     'binsize': 104,\n",
      "                     'chi2': 37.11230519039036,\n",
      "                     'degrees_of_freedom': 18,\n",
      "                     'p_value': 0.005066786334554883}}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    # --- core data ---\n",
    "    \"df\": apple,   # your cleaned OHLC dataframe with DATE, OPEN, CLOSE\n",
    "\n",
    "    # --- rolling window + model search ---\n",
    "    \"VALID_WEEKS\": 52,\n",
    "    \"depth_grid\": [2, 3, 4, 5, 6],\n",
    "    \"leaf_grid\": [2, 3, 4, 5, 6],\n",
    "    \"thresholds_tested\": np.linspace(0.01, 0.99, 99),\n",
    "\n",
    "    \"FIXED\": {\n",
    "        \"criterion\": \"entropy\",\n",
    "        \"min_samples_split\": 6,\n",
    "        \"class_weight\": \"balanced\",\n",
    "        \"random_state\": 42,\n",
    "    },\n",
    "\n",
    "    # --- scoring weights ---\n",
    "    \"alpha_p\": 1.0,\n",
    "    \"alpha_c\": 0.01,\n",
    "    \"p_min\": 0.55,\n",
    "    \"c_min\": 0.10,\n",
    "\n",
    "    # --- Monte Carlo (historical + future) ---\n",
    "    \"n_subsets\": 18,\n",
    "    \"n_trajectories\": 100000,\n",
    "    \"n_weeks\": 100,\n",
    "    \"initial_bank\": 100.0,\n",
    "    \"upper_thresh\": 200.0,\n",
    "    \"lower_thresh\": 60.0,\n",
    "    \"rng_seed\": 42,\n",
    "\n",
    "    # --- uniformity test ---\n",
    "    \"uniformity_binsize\": 104,\n",
    "}\n",
    "\n",
    "results = full_strategy_pipeline(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c3359",
   "metadata": {},
   "source": [
    "# few changes made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_strategy_pipeline(params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Weekly trading pipeline:\n",
    "      - Builds weekly Tue→Thu dataset with thu/tue multipliers.\n",
    "      - Rolling train/validate/test Decision Tree with threshold tuning.\n",
    "      - Computes confusion counts, precision, chattiness, correctness.\n",
    "      - Runs test for randomness of correctness.\n",
    "      - Uniformity (chi-square) across time with a chosen bin size.\n",
    "      - Historical Monte Carlo using empirical TP/FP multipliers.\n",
    "      - Future Monte Carlo using last subset.\n",
    "      - Baseline comparisons.\n",
    "      - Returns a report-card dictionary + prints it.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # --------------------------- INPUTS --------------------------\n",
    "    # ============================================================\n",
    "\n",
    "    df = params[\"df\"]\n",
    "\n",
    "    VALID_WEEKS       = params[\"VALID_WEEKS\"]\n",
    "    depth_grid        = params[\"depth_grid\"]\n",
    "    leaf_grid         = params[\"leaf_grid\"]\n",
    "    thresholds_tested = params[\"thresholds_tested\"]\n",
    "    FIXED             = params[\"FIXED\"]\n",
    "\n",
    "    alpha_p = params[\"alpha_p\"]\n",
    "    alpha_c = params[\"alpha_c\"]\n",
    "    p_min   = params[\"p_min\"]\n",
    "    c_min   = params[\"c_min\"]\n",
    "\n",
    "    n_subsets      = params[\"n_subsets\"]\n",
    "    n_trajectories = params[\"n_trajectories\"]\n",
    "    n_weeks        = params[\"n_weeks\"]\n",
    "    initial_bank   = params[\"initial_bank\"]\n",
    "    upper_thresh   = params[\"upper_thresh\"]\n",
    "    lower_thresh   = params[\"lower_thresh\"]\n",
    "    rng_seed       = params[\"rng_seed\"]\n",
    "\n",
    "    uniformity_binsize = params[\"uniformity_binsize\"]\n",
    "\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    # ============================================================\n",
    "    # ---------- PART I: CLEANING + WEEKLY DATASET ---------------\n",
    "    # ============================================================\n",
    "\n",
    "    df = df.sort_values(\"DATE\").reset_index(drop=True)\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "\n",
    "    df[\"normalized_close\"] = (\n",
    "        (df[\"CLOSE\"] - df[\"CLOSE\"].expanding().mean().shift(1))\n",
    "        / df[\"CLOSE\"].expanding().std(ddof=0).shift(1)\n",
    "    )\n",
    "    df[\"normalized_open\"] = (\n",
    "        (df[\"OPEN\"] - df[\"OPEN\"].expanding().mean().shift(1))\n",
    "        / df[\"OPEN\"].expanding().std(ddof=0).shift(1)\n",
    "    )\n",
    "\n",
    "    df[\"weekday\"] = df[\"DATE\"].dt.weekday\n",
    "    df[\"week\"]    = df[\"DATE\"].dt.to_period(\"W-SUN\")\n",
    "\n",
    "    tue_open = (\n",
    "        df[df[\"weekday\"] == 1]\n",
    "        .groupby(\"week\")[\"OPEN\"]\n",
    "        .first()\n",
    "        .rename(\"tue_open\")\n",
    "    )\n",
    "    thu_open = (\n",
    "        df[df[\"weekday\"] == 3]\n",
    "        .groupby(\"week\")[\"OPEN\"]\n",
    "        .first()\n",
    "        .rename(\"thu_open\")\n",
    "    )\n",
    "\n",
    "    weekly = pd.concat([tue_open, thu_open], axis=1)\n",
    "\n",
    "    weekly[\"thu/tue\"] = weekly[\"thu_open\"] / weekly[\"tue_open\"]\n",
    "    weekly[\"net%\"] = (weekly[\"thu/tue\"] - 1.0) * 100.0\n",
    "    weekly[\"week_type\"] = (weekly[\"thu/tue\"] > 1.0).astype(int)\n",
    "\n",
    "    norm_tue_open = (\n",
    "        df[df[\"weekday\"] == 1]\n",
    "        .set_index(\"week\")[\"normalized_open\"]\n",
    "        .rename(\"Norm_Tue_Open\")\n",
    "    )\n",
    "    norm_prev_thu_open = (\n",
    "        df[df[\"weekday\"] == 3]\n",
    "        .set_index(\"week\")[\"normalized_open\"]\n",
    "        .rename(\"Norm_PrevThu_Open\")\n",
    "        .shift(1)\n",
    "    )\n",
    "    norm_prev_fri_open = (\n",
    "        df[df[\"weekday\"] == 4]\n",
    "        .set_index(\"week\")[\"normalized_open\"]\n",
    "        .rename(\"Norm_PrevFri_Open\")\n",
    "        .shift(1)\n",
    "    )\n",
    "\n",
    "    weekly_full_norm = (\n",
    "        weekly.copy()\n",
    "        .join(norm_tue_open, how=\"left\")\n",
    "        .join(norm_prev_thu_open, how=\"left\")\n",
    "        .join(norm_prev_fri_open, how=\"left\")\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "    features = [\"Norm_PrevThu_Open\", \"Norm_PrevFri_Open\", \"Norm_Tue_Open\"]\n",
    "    target   = \"week_type\"\n",
    "\n",
    "    # ============================================================\n",
    "    # ---------- PART II: ROLLING TRAIN-VAL-TEST -----------------\n",
    "    # ============================================================\n",
    "\n",
    "    def precision(tp, fp):\n",
    "        return tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "\n",
    "    def chattiness(tp, fp, fn):\n",
    "        return (tp + fp) / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    def model_score(tp, fp, fn):\n",
    "        P = precision(tp, fp)\n",
    "        C = chattiness(tp, fp, fn)\n",
    "        s = np.exp(alpha_p * (P - p_min) + alpha_c * (C - c_min))\n",
    "        return 0.0 if np.isnan(s) or np.isinf(s) else float(s)\n",
    "\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    TP = TN = FP = FN = 0\n",
    "    weekly_best = []\n",
    "\n",
    "    for t in tqdm(range(VALID_WEEKS + 1, len(weekly_full_norm)), desc=\"Rolling simulation\"):\n",
    "\n",
    "        val_start = max(0, t - VALID_WEEKS)\n",
    "        training   = weekly_full_norm.iloc[:val_start]\n",
    "        validation = weekly_full_norm.iloc[val_start:t]\n",
    "        test       = weekly_full_norm.iloc[[t]]\n",
    "\n",
    "        if len(training[target].unique()) < 2:\n",
    "            continue\n",
    "\n",
    "        train_X, train_y = training[features], training[target]\n",
    "        val_X, val_y     = validation[features], validation[target]\n",
    "        test_X, test_y   = test[features], test[target]\n",
    "\n",
    "        best_score  = -np.inf\n",
    "        best_params = None\n",
    "        best_model  = None\n",
    "\n",
    "        for depth, leaf in product(depth_grid, leaf_grid):\n",
    "            model = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=leaf, **FIXED)\n",
    "            model.fit(train_X, train_y)\n",
    "            probs_val = model.predict_proba(val_X)[:, 1]\n",
    "\n",
    "            for thr in thresholds_tested:\n",
    "                preds_val = (probs_val > thr).astype(int)\n",
    "                tp = ((preds_val == 1) & (val_y == 1)).sum()\n",
    "                fp = ((preds_val == 1) & (val_y == 0)).sum()\n",
    "                fn = ((preds_val == 0) & (val_y == 1)).sum()\n",
    "                sc = model_score(tp, fp, fn)\n",
    "                if sc > best_score:\n",
    "                    best_score  = sc\n",
    "                    best_params = (depth, leaf, thr)\n",
    "                    best_model  = model\n",
    "\n",
    "        best_depth, best_leaf, best_thr = best_params\n",
    "\n",
    "        p_hat = best_model.predict_proba(test_X)[0, 1]\n",
    "        pred  = int(p_hat > best_thr)\n",
    "        true  = int(test_y.iloc[0])\n",
    "\n",
    "        if pred == 1 and true == 1:\n",
    "            TP += 1; outcome = \"TP\"\n",
    "        elif pred == 0 and true == 0:\n",
    "            TN += 1; outcome = \"TN\"\n",
    "        elif pred == 1 and true == 0:\n",
    "            FP += 1; outcome = \"FP\"\n",
    "        else:\n",
    "            FN += 1; outcome = \"FN\"\n",
    "\n",
    "        weekly_best.append({\n",
    "            \"Week\": t,\n",
    "            \"True_Label\": true,\n",
    "            \"Pred_Label\": pred,\n",
    "            \"Outcome\": outcome,\n",
    "            \"thu_tue\": float(test[\"thu/tue\"].iloc[0]),\n",
    "        })\n",
    "\n",
    "    df_final = pd.DataFrame(weekly_best)\n",
    "\n",
    "    # ============================================================\n",
    "    # ---------------- BASIC METRICS + TESTS ---------------------\n",
    "    # ============================================================\n",
    "\n",
    "    total = TP + TN + FP + FN\n",
    "    prec_overall = precision(TP, FP)\n",
    "    chat_overall = chattiness(TP, FP, FN)\n",
    "    correctness_rate = (TP + TN) / total\n",
    "\n",
    "    pct_fp_positive = FP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "\n",
    "    df_final[\"correct\"] = (df_final[\"True_Label\"] == df_final[\"Pred_Label\"]).astype(int)\n",
    "    z_runs, p_runs = runstest_1samp(df_final[\"correct\"], correction=False)\n",
    "\n",
    "    randomness_test = {\n",
    "        \"H0\": \"Correctness is random in time.\",\n",
    "        \"z\": float(z_runs),\n",
    "        \"p\": float(p_runs)\n",
    "    }\n",
    "\n",
    "    df_final[\"chunk\"] = df_final.index // uniformity_binsize\n",
    "    chi2, p_chi, dof, _ = chi2_contingency(pd.crosstab(df_final[\"chunk\"], df_final[\"correct\"]))\n",
    "\n",
    "    uniformity_test = {\n",
    "        \"chi2\": float(chi2),\n",
    "        \"p\": float(p_chi),\n",
    "        \"dof\": int(dof),\n",
    "        \"binsize\": uniformity_binsize,\n",
    "    }\n",
    "\n",
    "    # ---------- longest streaks (Option 2) ----------\n",
    "    def longest_streak(seq, label):\n",
    "        best = 0\n",
    "        cur = 0\n",
    "        for x in seq:\n",
    "            if x == label:\n",
    "                cur += 1\n",
    "                best = max(best, cur)\n",
    "            else:\n",
    "                cur = 0\n",
    "        return best\n",
    "\n",
    "    longest_tp = longest_streak(df_final[\"Outcome\"], \"TP\")\n",
    "    longest_fp = longest_streak(df_final[\"Outcome\"], \"FP\")\n",
    "\n",
    "    # ============================================================\n",
    "    # ----------------- PART III: HISTORICAL MC ------------------\n",
    "    # ============================================================\n",
    "\n",
    "    outcomes_arr = np.array([\"TP\", \"FP\", \"FN\", \"TN\"])\n",
    "\n",
    "    def build_samplers(tp_vals, fp_vals):\n",
    "        tp_sorted = np.sort(tp_vals)\n",
    "        fp_sorted = np.sort(fp_vals)\n",
    "\n",
    "        tp_cdf = np.arange(1, len(tp_sorted)+1)/len(tp_sorted)\n",
    "        fp_cdf = np.arange(1, len(fp_sorted)+1)/len(fp_sorted)\n",
    "\n",
    "        def sample_tp():\n",
    "            u = rng.random()\n",
    "            return tp_sorted[np.searchsorted(tp_cdf, u)]\n",
    "\n",
    "        def sample_fp():\n",
    "            u = rng.random()\n",
    "            return fp_sorted[np.searchsorted(fp_cdf, u)]\n",
    "\n",
    "        return sample_tp, sample_fp\n",
    "\n",
    "    def run_mc_block(p, sample_tp, sample_fp):\n",
    "        cdf = np.cumsum(p)\n",
    "        final = np.empty(n_trajectories)\n",
    "\n",
    "        for i in range(n_trajectories):\n",
    "            bank = initial_bank\n",
    "            for _ in range(n_weeks):\n",
    "                r = rng.random()\n",
    "                idx = np.searchsorted(cdf, r)\n",
    "                outcome = outcomes_arr[idx]\n",
    "\n",
    "                if outcome == \"TP\":\n",
    "                    bank *= sample_tp()\n",
    "                elif outcome == \"FP\":\n",
    "                    bank *= sample_fp()\n",
    "\n",
    "                if bank >= upper_thresh or bank <= lower_thresh:\n",
    "                    break\n",
    "\n",
    "            final[i] = bank\n",
    "        return final\n",
    "\n",
    "    def run_actual(sub):\n",
    "        bank = initial_bank\n",
    "        for _, row in sub.iterrows():\n",
    "            if row[\"Outcome\"] in (\"TP\", \"FP\"):\n",
    "                bank *= row[\"thu_tue\"]\n",
    "            if bank >= upper_thresh or bank <= lower_thresh:\n",
    "                break\n",
    "        return bank\n",
    "\n",
    "    raw_subsets = np.array_split(df_final, n_subsets)\n",
    "    subsets = [s for s in raw_subsets if len(s) > 0]\n",
    "\n",
    "    all_sims = []\n",
    "    actual_balances = []\n",
    "    null_percentiles = []\n",
    "    valid_mc_subsets = []\n",
    "\n",
    "    for sub in subsets:\n",
    "        tp_vals = sub.loc[sub[\"Outcome\"]==\"TP\", \"thu_tue\"].values\n",
    "        fp_vals = sub.loc[sub[\"Outcome\"]==\"FP\", \"thu_tue\"].values\n",
    "\n",
    "        if len(tp_vals) < 2 or len(fp_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        sample_tp, sample_fp = build_samplers(tp_vals, fp_vals)\n",
    "        p = sub[\"Outcome\"].value_counts(normalize=True).reindex(outcomes_arr, fill_value=0).values\n",
    "\n",
    "        sims = run_mc_block(p, sample_tp, sample_fp)\n",
    "        all_sims.append(sims)\n",
    "\n",
    "        actual = run_actual(sub)\n",
    "        actual_balances.append(actual)\n",
    "\n",
    "        null_percentiles.append(np.mean(sims <= actual))\n",
    "        valid_mc_subsets.append(sub)\n",
    "\n",
    "    sim_all = np.concatenate(all_sims)\n",
    "    actual_balances = np.array(actual_balances)\n",
    "\n",
    "    simulated_mean = float(sim_all.mean())\n",
    "    simulated_median = float(np.median(sim_all))\n",
    "    ks_d, ks_p = ks_2samp(actual_balances, sim_all)\n",
    "    avg_null = float(np.mean(null_percentiles))\n",
    "\n",
    "    # ============================================================\n",
    "    # --------------------- PART IV: FUTURE MC -------------------\n",
    "    # ============================================================\n",
    "\n",
    "    last = valid_mc_subsets[-1]\n",
    "\n",
    "    tp_last = last.loc[last[\"Outcome\"]==\"TP\", \"thu_tue\"].values\n",
    "    fp_last = last.loc[last[\"Outcome\"]==\"FP\", \"thu_tue\"].values\n",
    "\n",
    "    sample_tp_f, sample_fp_f = build_samplers(tp_last, fp_last)\n",
    "    p_last = last[\"Outcome\"].value_counts(normalize=True).reindex(outcomes_arr, fill_value=0).values\n",
    "\n",
    "    fut = run_mc_block(p_last, sample_tp_f, sample_fp_f)\n",
    "\n",
    "    future_mean = float(fut.mean())\n",
    "    future_median = float(np.median(fut))\n",
    "    prob_above_initial = float(np.mean(fut > initial_bank))\n",
    "    prob_success = float(np.mean(fut >= upper_thresh))\n",
    "    prob_failure = float(np.mean(fut <= lower_thresh))\n",
    "    prob_uncertain = float(1 - prob_success - prob_failure)\n",
    "\n",
    "    # ============================================================\n",
    "    # --------------------- INTERNAL METRICS ---------------------\n",
    "    # ============================================================\n",
    "\n",
    "    TP_vals = df_final.loc[df_final[\"Outcome\"]==\"TP\", \"thu_tue\"].values\n",
    "    FP_vals = df_final.loc[df_final[\"Outcome\"]==\"FP\", \"thu_tue\"].values\n",
    "\n",
    "    precision_on_trades = float(len(TP_vals)/(len(TP_vals)+len(FP_vals))) if len(TP_vals)+len(FP_vals)>0 else 0.0\n",
    "    trade_frequency = float((len(TP_vals)+len(FP_vals))/len(df_final))\n",
    "\n",
    "    # Asymmetry using percentage return\n",
    "    tp_pct = (TP_vals - 1) * 100\n",
    "    fp_pct = (FP_vals - 1) * 100\n",
    "    mistake_asymmetry = float(tp_pct.mean() - fp_pct.mean()) if len(tp_pct)>0 and len(fp_pct)>0 else np.nan\n",
    "\n",
    "    gains  = df_final.loc[df_final[\"thu_tue\"]>1, \"thu_tue\"].values\n",
    "    losses = df_final.loc[df_final[\"thu_tue\"]<1, \"thu_tue\"].values\n",
    "\n",
    "    if len(gains) > 0 and len(losses) > 0:\n",
    "        macro = float(gains.mean() / abs(losses.mean()))\n",
    "        micro = float((gains[:len(losses)] / losses[:len(gains)]).mean())\n",
    "        gap   = float(macro - micro)\n",
    "    else:\n",
    "        macro = micro = gap = np.nan\n",
    "\n",
    "    # ============================================================\n",
    "    # -------------------- BASELINE COMPARISON -------------------\n",
    "    # ============================================================\n",
    "\n",
    "    ratio_always, ratio_random, ratio_alt, ratio_weighted = [], [], [], []\n",
    "\n",
    "    for sub in subsets:\n",
    "\n",
    "        model_bal = run_actual(sub)\n",
    "\n",
    "        # always trade\n",
    "        b = initial_bank\n",
    "        for r in sub[\"thu_tue\"]:\n",
    "            b *= r\n",
    "            if b >= upper_thresh or b <= lower_thresh:\n",
    "                break\n",
    "        ratio_always.append(model_bal/b if b!=0 else np.nan)\n",
    "\n",
    "        # random with same chattiness\n",
    "        ch_prob = len(sub.loc[sub[\"Outcome\"].isin([\"TP\",\"FP\"])])/len(sub)\n",
    "        b = initial_bank\n",
    "        for r in sub[\"thu_tue\"]:\n",
    "            if rng.random()<ch_prob:\n",
    "                b*=r\n",
    "        ratio_random.append(model_bal/b if b!=0 else np.nan)\n",
    "\n",
    "        # alternate week\n",
    "        b = initial_bank\n",
    "        for i,r in enumerate(sub[\"thu_tue\"]):\n",
    "            if i%2==0:\n",
    "                b*=r\n",
    "        ratio_alt.append(model_bal/b if b!=0 else np.nan)\n",
    "\n",
    "        # weighted coin\n",
    "        good_rate = float((sub[\"thu_tue\"]>1).mean())\n",
    "        b = initial_bank\n",
    "        for r in sub[\"thu_tue\"]:\n",
    "            if rng.random()<good_rate:\n",
    "                b*=r\n",
    "        ratio_weighted.append(model_bal/b if b!=0 else np.nan)\n",
    "\n",
    "    # ============================================================\n",
    "    # ------------------------ FINAL REPORT ----------------------\n",
    "    # ============================================================\n",
    "\n",
    "    report = {\n",
    "        \"historical_mc\": {\n",
    "            \"simulated_mean\": simulated_mean,\n",
    "            \"simulated_median\": simulated_median,\n",
    "            \"ks_distance\": float(ks_d),\n",
    "            \"ks_p_value\": float(ks_p),\n",
    "            \"average_null_percentile\": avg_null,\n",
    "        },\n",
    "        \"future_mc\": {\n",
    "            \"future_mean\": future_mean,\n",
    "            \"future_median\": future_median,\n",
    "            \"prob_above_initial\": prob_above_initial,\n",
    "            \"prob_success\": prob_success,\n",
    "            \"prob_failure\": prob_failure,\n",
    "            \"prob_uncertain\": prob_uncertain,\n",
    "        },\n",
    "        \"internal_metrics\": {\n",
    "            \"precision_overall\": prec_overall,\n",
    "            \"chattiness_overall\": chat_overall,\n",
    "            \"correctness_rate\": correctness_rate,\n",
    "            \"precision_on_trades\": precision_on_trades,\n",
    "            \"trade_frequency\": trade_frequency,\n",
    "            \"mistake_asymmetry_%\": mistake_asymmetry,\n",
    "            \"macro_return_ratio\": macro,\n",
    "            \"micro_return_ratio\": micro,\n",
    "            \"return_ratio_gap\": gap,\n",
    "            \"longest_TP_streak\": longest_tp,\n",
    "            \"longest_FP_streak\": longest_fp,\n",
    "            \"%FP_when_predicted_positive\": pct_fp_positive,\n",
    "        },\n",
    "        \"baseline_comparison\": {\n",
    "            \"vs_always_trade\": float(np.nanmean(ratio_always)),\n",
    "            \"vs_random_trader\": float(np.nanmean(ratio_random)),\n",
    "            \"vs_alternate_trader\": float(np.nanmean(ratio_alt)),\n",
    "            \"vs_weighted_coin\": float(np.nanmean(ratio_weighted)),\n",
    "        },\n",
    "        \"uniformity_test\": uniformity_test,\n",
    "        \"randomness_test\": randomness_test,\n",
    "    }\n",
    "\n",
    "    pprint(report)\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9334f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling simulation:   0%|          | 0/1909 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling simulation: 100%|██████████| 1909/1909 [46:13<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'baseline_comparison': {'vs_alternate_trader': 1.1636995183050196,\n",
      "                         'vs_always_trade': 1.1551936421319584,\n",
      "                         'vs_random_trader': 1.2102404399011697,\n",
      "                         'vs_weighted_coin': 1.1866106547622692},\n",
      " 'future_mc': {'future_mean': 130.23050665682405,\n",
      "               'future_median': 128.22582303621516,\n",
      "               'prob_above_initial': 0.89096,\n",
      "               'prob_failure': 0.00039,\n",
      "               'prob_success': 0.01272,\n",
      "               'prob_uncertain': 0.98689},\n",
      " 'historical_mc': {'average_null_percentile': 0.5226894444444444,\n",
      "                   'ks_distance': 0.09601944444444446,\n",
      "                   'ks_p_value': 0.9905969629687478,\n",
      "                   'simulated_mean': 128.24714802704327,\n",
      "                   'simulated_median': 121.1748874333227},\n",
      " 'internal_metrics': {'%FP_when_predicted_positive': 0.4537987679671458,\n",
      "                      'chattiness_overall': 0.9938775510204082,\n",
      "                      'correctness_rate': 0.5332983744100682,\n",
      "                      'longest_FP_streak': 8,\n",
      "                      'longest_TP_streak': 9,\n",
      "                      'macro_return_ratio': 1.0574821549461133,\n",
      "                      'micro_return_ratio': 1.0594461540759437,\n",
      "                      'mistake_asymmetry_%': 5.10152878811977,\n",
      "                      'precision_on_trades': 0.5462012320328542,\n",
      "                      'precision_overall': 0.5462012320328542,\n",
      "                      'return_ratio_gap': -0.0019639991298303627,\n",
      "                      'trade_frequency': 0.5107498689040377},\n",
      " 'randomness_test': {'H0': 'Correctness is random in time.',\n",
      "                     'p': 0.046467381868778274,\n",
      "                     'z': -1.991122859601576},\n",
      " 'uniformity_test': {'binsize': 104,\n",
      "                     'chi2': 37.11230519039036,\n",
      "                     'dof': 18,\n",
      "                     'p': 0.005066786334554883}}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    # --- core data ---\n",
    "    \"df\": apple,   # your cleaned OHLC dataframe with DATE, OPEN, CLOSE\n",
    "\n",
    "    # --- rolling window + model search ---\n",
    "    \"VALID_WEEKS\": 52,\n",
    "    \"depth_grid\": [2, 3, 4, 5, 6],\n",
    "    \"leaf_grid\": [2, 3, 4, 5, 6],\n",
    "    \"thresholds_tested\": np.linspace(0.01, 0.99, 99),\n",
    "\n",
    "    \"FIXED\": {\n",
    "        \"criterion\": \"entropy\",\n",
    "        \"min_samples_split\": 6,\n",
    "        \"class_weight\": \"balanced\",\n",
    "        \"random_state\": 42,\n",
    "    },\n",
    "\n",
    "    # --- scoring weights ---\n",
    "    \"alpha_p\": 1.0,\n",
    "    \"alpha_c\": 0.01,\n",
    "    \"p_min\": 0.55,\n",
    "    \"c_min\": 0.10,\n",
    "\n",
    "    # --- Monte Carlo (historical + future) ---\n",
    "    \"n_subsets\": 18,\n",
    "    \"n_trajectories\": 100000,\n",
    "    \"n_weeks\": 100,\n",
    "    \"initial_bank\": 100.0,\n",
    "    \"upper_thresh\": 200.0,\n",
    "    \"lower_thresh\": 60.0,\n",
    "    \"rng_seed\": 42,\n",
    "\n",
    "    # --- uniformity test ---\n",
    "    \"uniformity_binsize\": 104,\n",
    "}\n",
    "\n",
    "results = full_strategy_pipeline(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091138ba",
   "metadata": {},
   "source": [
    "# Final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_strategy_pipeline(params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Weekly trading pipeline:\n",
    "      - Builds weekly Tue→Thu dataset with thu/tue multipliers.\n",
    "      - Rolling train/validate/test Decision Tree with threshold tuning.\n",
    "      - Computes confusion counts, precision, chattiness, correctness.\n",
    "      - Runs test for randomness of correctness.\n",
    "      - Uniformity (chi-square) across time with a chosen bin size.\n",
    "      - Historical Monte Carlo using empirical TP/FP multipliers.\n",
    "      - Future Monte Carlo using last subset.\n",
    "      - Baseline comparisons.\n",
    "      - Returns a report-card dictionary + prints it.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # --------------------------- INPUTS --------------------------\n",
    "    # ============================================================\n",
    "\n",
    "    df = params[\"df\"]\n",
    "\n",
    "    VALID_WEEKS       = params[\"VALID_WEEKS\"]\n",
    "    depth_grid        = params[\"depth_grid\"]\n",
    "    leaf_grid         = params[\"leaf_grid\"]\n",
    "    thresholds_tested = params[\"thresholds_tested\"]\n",
    "    FIXED             = params[\"FIXED\"]\n",
    "\n",
    "    alpha_p = params[\"alpha_p\"]\n",
    "    alpha_c = params[\"alpha_c\"]\n",
    "    p_min   = params[\"p_min\"]\n",
    "    c_min   = params[\"c_min\"]\n",
    "\n",
    "    n_subsets      = params[\"n_subsets\"]\n",
    "    n_trajectories = params[\"n_trajectories\"]\n",
    "    n_weeks        = params[\"n_weeks\"]\n",
    "    initial_bank   = params[\"initial_bank\"]\n",
    "    upper_thresh   = params[\"upper_thresh\"]\n",
    "    lower_thresh   = params[\"lower_thresh\"]\n",
    "    rng_seed       = params[\"rng_seed\"]\n",
    "\n",
    "    uniformity_binsize = params[\"uniformity_binsize\"]\n",
    "\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    # ============================================================\n",
    "    # ---------- PART I: CLEANING + WEEKLY DATASET ---------------\n",
    "    # ============================================================\n",
    "\n",
    "    df = df.sort_values(\"DATE\").reset_index(drop=True)\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "\n",
    "    df[\"normalized_close\"] = (\n",
    "        (df[\"CLOSE\"] - df[\"CLOSE\"].expanding().mean().shift(1))\n",
    "        / df[\"CLOSE\"].expanding().std(ddof=0).shift(1)\n",
    "    )\n",
    "    df[\"normalized_open\"] = (\n",
    "        (df[\"OPEN\"] - df[\"OPEN\"].expanding().mean().shift(1))\n",
    "        / df[\"OPEN\"].expanding().std(ddof=0).shift(1)\n",
    "    )\n",
    "\n",
    "    df[\"weekday\"] = df[\"DATE\"].dt.weekday\n",
    "    df[\"week\"]    = df[\"DATE\"].dt.to_period(\"W-SUN\")\n",
    "\n",
    "    tue_open = (\n",
    "        df[df[\"weekday\"] == 1]\n",
    "        .groupby(\"week\")[\"OPEN\"]\n",
    "        .first()\n",
    "        .rename(\"tue_open\")\n",
    "    )\n",
    "    thu_open = (\n",
    "        df[df[\"weekday\"] == 3]\n",
    "        .groupby(\"week\")[\"OPEN\"]\n",
    "        .first()\n",
    "        .rename(\"thu_open\")\n",
    "    )\n",
    "\n",
    "    weekly = pd.concat([tue_open, thu_open], axis=1)\n",
    "\n",
    "    weekly[\"thu/tue\"] = weekly[\"thu_open\"] / weekly[\"tue_open\"]\n",
    "    weekly[\"net%\"] = (weekly[\"thu/tue\"] - 1.0) * 100.0\n",
    "    weekly[\"week_type\"] = (weekly[\"thu/tue\"] > 1.0).astype(int)\n",
    "\n",
    "    norm_tue_open = (\n",
    "        df[df[\"weekday\"] == 1]\n",
    "        .set_index(\"week\")[\"normalized_open\"]\n",
    "        .rename(\"Norm_Tue_Open\")\n",
    "    )\n",
    "    norm_prev_thu_open = (\n",
    "        df[df[\"weekday\"] == 3]\n",
    "        .set_index(\"week\")[\"normalized_open\"]\n",
    "        .rename(\"Norm_PrevThu_Open\")\n",
    "        .shift(1)\n",
    "    )\n",
    "    norm_prev_fri_open = (\n",
    "        df[df[\"weekday\"] == 4]\n",
    "        .set_index(\"week\")[\"normalized_open\"]\n",
    "        .rename(\"Norm_PrevFri_Open\")\n",
    "        .shift(1)\n",
    "    )\n",
    "\n",
    "    weekly_full_norm = (\n",
    "        weekly.copy()\n",
    "        .join(norm_tue_open, how=\"left\")\n",
    "        .join(norm_prev_thu_open, how=\"left\")\n",
    "        .join(norm_prev_fri_open, how=\"left\")\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "    features = [\"Norm_PrevThu_Open\", \"Norm_PrevFri_Open\", \"Norm_Tue_Open\"]\n",
    "    target   = \"week_type\"\n",
    "\n",
    "    # ============================================================\n",
    "    # ---------- PART II: ROLLING TRAIN-VAL-TEST -----------------\n",
    "    # ============================================================\n",
    "\n",
    "    def precision(tp, fp):\n",
    "        return tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "\n",
    "    def chattiness(tp, fp, fn):\n",
    "        return (tp + fp) / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    def model_score(tp, fp, fn):\n",
    "        P = precision(tp, fp)\n",
    "        C = chattiness(tp, fp, fn)\n",
    "        s = np.exp(alpha_p * (P - p_min) + alpha_c * (C - c_min))\n",
    "        return 0.0 if np.isnan(s) or np.isinf(s) else float(s)\n",
    "\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    TP = TN = FP = FN = 0\n",
    "    weekly_best = []\n",
    "\n",
    "    for t in tqdm(range(VALID_WEEKS + 1, len(weekly_full_norm)), desc=\"Rolling simulation\"):\n",
    "\n",
    "        val_start = max(0, t - VALID_WEEKS)\n",
    "        training   = weekly_full_norm.iloc[:val_start]\n",
    "        validation = weekly_full_norm.iloc[val_start:t]\n",
    "        test       = weekly_full_norm.iloc[[t]]\n",
    "\n",
    "        if len(training[target].unique()) < 2:\n",
    "            continue\n",
    "\n",
    "        train_X, train_y = training[features], training[target]\n",
    "        val_X, val_y     = validation[features], validation[target]\n",
    "        test_X, test_y   = test[features], test[target]\n",
    "\n",
    "        best_score  = -np.inf\n",
    "        best_params = None\n",
    "        best_model  = None\n",
    "\n",
    "        for depth, leaf in product(depth_grid, leaf_grid):\n",
    "            model = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=leaf, **FIXED)\n",
    "            model.fit(train_X, train_y)\n",
    "            probs_val = model.predict_proba(val_X)[:, 1]\n",
    "\n",
    "            for thr in thresholds_tested:\n",
    "                preds_val = (probs_val > thr).astype(int)\n",
    "                tp = ((preds_val == 1) & (val_y == 1)).sum()\n",
    "                fp = ((preds_val == 1) & (val_y == 0)).sum()\n",
    "                fn = ((preds_val == 0) & (val_y == 1)).sum()\n",
    "                sc = model_score(tp, fp, fn)\n",
    "                if sc > best_score:\n",
    "                    best_score  = sc\n",
    "                    best_params = (depth, leaf, thr)\n",
    "                    best_model  = model\n",
    "\n",
    "        best_depth, best_leaf, best_thr = best_params\n",
    "\n",
    "        p_hat = best_model.predict_proba(test_X)[0, 1]\n",
    "        pred  = int(p_hat > best_thr)\n",
    "        true  = int(test_y.iloc[0])\n",
    "\n",
    "        if pred == 1 and true == 1:\n",
    "            TP += 1; outcome = \"TP\"\n",
    "        elif pred == 0 and true == 0:\n",
    "            TN += 1; outcome = \"TN\"\n",
    "        elif pred == 1 and true == 0:\n",
    "            FP += 1; outcome = \"FP\"\n",
    "        else:\n",
    "            FN += 1; outcome = \"FN\"\n",
    "\n",
    "        weekly_best.append({\n",
    "            \"Week\": t,\n",
    "            \"True_Label\": true,\n",
    "            \"Pred_Label\": pred,\n",
    "            \"Outcome\": outcome,\n",
    "            \"thu_tue\": float(test[\"thu/tue\"].iloc[0]),\n",
    "        })\n",
    "\n",
    "    df_final = pd.DataFrame(weekly_best)\n",
    "\n",
    "    # ============================================================\n",
    "    # ---------------- BASIC METRICS + TESTS ---------------------\n",
    "    # ============================================================\n",
    "\n",
    "    total = TP + TN + FP + FN\n",
    "    prec_overall = precision(TP, FP)\n",
    "    chat_overall = chattiness(TP, FP, FN)\n",
    "    correctness_rate = (TP + TN) / total\n",
    "\n",
    "    pct_fp_positive = FP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "\n",
    "    df_final[\"correct\"] = (df_final[\"True_Label\"] == df_final[\"Pred_Label\"]).astype(int)\n",
    "    z_runs, p_runs = runstest_1samp(df_final[\"correct\"], correction=False)\n",
    "\n",
    "    randomness_test = {\n",
    "        \"H0\": \"Correctness is random in time.\",\n",
    "        \"z\": float(z_runs),\n",
    "        \"p\": float(p_runs)\n",
    "    }\n",
    "\n",
    "    df_final[\"chunk\"] = df_final.index // uniformity_binsize\n",
    "    chi2, p_chi, dof, _ = chi2_contingency(pd.crosstab(df_final[\"chunk\"], df_final[\"correct\"]))\n",
    "\n",
    "    uniformity_test = {\n",
    "        \"chi2\": float(chi2),\n",
    "        \"p\": float(p_chi),\n",
    "        \"dof\": int(dof),\n",
    "        \"binsize\": uniformity_binsize,\n",
    "    }\n",
    "\n",
    "    # ---------- longest streaks (Option 2) ----------\n",
    "    def longest_streak(seq, label):\n",
    "        best = 0\n",
    "        cur = 0\n",
    "        for x in seq:\n",
    "            if x == label:\n",
    "                cur += 1\n",
    "                best = max(best, cur)\n",
    "            else:\n",
    "                cur = 0\n",
    "        return best\n",
    "\n",
    "    longest_tp = longest_streak(df_final[\"Outcome\"], \"TP\")\n",
    "    longest_fp = longest_streak(df_final[\"Outcome\"], \"FP\")\n",
    "\n",
    "    # ============================================================\n",
    "    # ----------------- PART III: HISTORICAL MC ------------------\n",
    "    # ============================================================\n",
    "\n",
    "    outcomes_arr = np.array([\"TP\", \"FP\", \"FN\", \"TN\"])\n",
    "\n",
    "    def build_samplers(tp_vals, fp_vals):\n",
    "        tp_sorted = np.sort(tp_vals)\n",
    "        fp_sorted = np.sort(fp_vals)\n",
    "\n",
    "        tp_cdf = np.arange(1, len(tp_sorted)+1)/len(tp_sorted)\n",
    "        fp_cdf = np.arange(1, len(fp_sorted)+1)/len(fp_sorted)\n",
    "\n",
    "        def sample_tp():\n",
    "            u = rng.random()\n",
    "            return tp_sorted[np.searchsorted(tp_cdf, u)]\n",
    "\n",
    "        def sample_fp():\n",
    "            u = rng.random()\n",
    "            return fp_sorted[np.searchsorted(fp_cdf, u)]\n",
    "\n",
    "        return sample_tp, sample_fp\n",
    "\n",
    "    def run_mc_block(p, sample_tp, sample_fp):\n",
    "        cdf = np.cumsum(p)\n",
    "        final = np.empty(n_trajectories)\n",
    "\n",
    "        for i in range(n_trajectories):\n",
    "            bank = initial_bank\n",
    "            for _ in range(n_weeks):\n",
    "                r = rng.random()\n",
    "                idx = np.searchsorted(cdf, r)\n",
    "                outcome = outcomes_arr[idx]\n",
    "\n",
    "                if outcome == \"TP\":\n",
    "                    bank *= sample_tp()\n",
    "                elif outcome == \"FP\":\n",
    "                    bank *= sample_fp()\n",
    "\n",
    "                if bank >= upper_thresh or bank <= lower_thresh:\n",
    "                    break\n",
    "\n",
    "            final[i] = bank\n",
    "        return final\n",
    "\n",
    "    def run_actual(sub):\n",
    "        bank = initial_bank\n",
    "        for _, row in sub.iterrows():\n",
    "            if row[\"Outcome\"] in (\"TP\", \"FP\"):\n",
    "                bank *= row[\"thu_tue\"]\n",
    "            if bank >= upper_thresh or bank <= lower_thresh:\n",
    "                break\n",
    "        return bank\n",
    "\n",
    "    raw_subsets = np.array_split(df_final, n_subsets)\n",
    "    subsets = [s for s in raw_subsets if len(s) > 0]\n",
    "\n",
    "    all_sims = []\n",
    "    actual_balances = []\n",
    "    null_percentiles = []\n",
    "    valid_mc_subsets = []\n",
    "\n",
    "    for sub in subsets:\n",
    "        tp_vals = sub.loc[sub[\"Outcome\"]==\"TP\", \"thu_tue\"].values\n",
    "        fp_vals = sub.loc[sub[\"Outcome\"]==\"FP\", \"thu_tue\"].values\n",
    "\n",
    "        if len(tp_vals) < 2 or len(fp_vals) < 2:\n",
    "            continue\n",
    "\n",
    "        sample_tp, sample_fp = build_samplers(tp_vals, fp_vals)\n",
    "        p = sub[\"Outcome\"].value_counts(normalize=True).reindex(outcomes_arr, fill_value=0).values\n",
    "\n",
    "        sims = run_mc_block(p, sample_tp, sample_fp)\n",
    "        all_sims.append(sims)\n",
    "\n",
    "        actual = run_actual(sub)\n",
    "        actual_balances.append(actual)\n",
    "\n",
    "        null_percentiles.append(np.mean(sims <= actual))\n",
    "        valid_mc_subsets.append(sub)\n",
    "\n",
    "    sim_all = np.concatenate(all_sims)\n",
    "    actual_balances = np.array(actual_balances)\n",
    "\n",
    "    simulated_mean = float(sim_all.mean())\n",
    "    simulated_median = float(np.median(sim_all))\n",
    "    ks_d, ks_p = ks_2samp(actual_balances, sim_all)\n",
    "    avg_null = float(np.mean(null_percentiles))\n",
    "\n",
    "    # ============================================================\n",
    "    # --------------------- PART IV: FUTURE MC -------------------\n",
    "    # ============================================================\n",
    "\n",
    "    last = valid_mc_subsets[-1]\n",
    "\n",
    "    tp_last = last.loc[last[\"Outcome\"]==\"TP\", \"thu_tue\"].values\n",
    "    fp_last = last.loc[last[\"Outcome\"]==\"FP\", \"thu_tue\"].values\n",
    "\n",
    "    sample_tp_f, sample_fp_f = build_samplers(tp_last, fp_last)\n",
    "    p_last = last[\"Outcome\"].value_counts(normalize=True).reindex(outcomes_arr, fill_value=0).values\n",
    "\n",
    "    fut = run_mc_block(p_last, sample_tp_f, sample_fp_f)\n",
    "\n",
    "    future_mean = float(fut.mean())\n",
    "    future_median = float(np.median(fut))\n",
    "    prob_above_initial = float(np.mean(fut > initial_bank))\n",
    "    prob_success = float(np.mean(fut >= upper_thresh))\n",
    "    prob_failure = float(np.mean(fut <= lower_thresh))\n",
    "    prob_uncertain = float(1 - prob_success - prob_failure)\n",
    "\n",
    "    # ============================================================\n",
    "    # --------------------- INTERNAL METRICS ---------------------\n",
    "    # ============================================================\n",
    "\n",
    "    TP_vals = df_final.loc[df_final[\"Outcome\"]==\"TP\", \"thu_tue\"].values\n",
    "    FP_vals = df_final.loc[df_final[\"Outcome\"]==\"FP\", \"thu_tue\"].values\n",
    "\n",
    "    precision_on_trades = float(len(TP_vals)/(len(TP_vals)+len(FP_vals))) if len(TP_vals)+len(FP_vals)>0 else 0.0\n",
    "    trade_frequency = float((len(TP_vals)+len(FP_vals))/len(df_final))\n",
    "\n",
    "    # Asymmetry using percentage return\n",
    "    tp_pct = (TP_vals - 1) * 100\n",
    "    fp_pct = (FP_vals - 1) * 100\n",
    "    mistake_asymmetry = float(tp_pct.mean() + fp_pct.mean()) if len(tp_pct)>0 and len(fp_pct)>0 else np.nan\n",
    "\n",
    "    gains  = df_final.loc[df_final[\"thu_tue\"]>1, \"thu_tue\"].values\n",
    "    losses = df_final.loc[df_final[\"thu_tue\"]<1, \"thu_tue\"].values\n",
    "\n",
    "    if len(gains) > 0 and len(losses) > 0:\n",
    "        macro = float(gains.mean() / abs(losses.mean()))\n",
    "        micro = float((gains[:len(losses)] / losses[:len(gains)]).mean())\n",
    "        gap   = float(macro - micro)\n",
    "    else:\n",
    "        macro = micro = gap = np.nan\n",
    "\n",
    "    # ============================================================\n",
    "    # -------------------- BASELINE COMPARISON -------------------\n",
    "    # ============================================================\n",
    "\n",
    "    ratio_always, ratio_random, ratio_alt, ratio_weighted = [], [], [], []\n",
    "\n",
    "    for sub in subsets:\n",
    "\n",
    "        model_bal = run_actual(sub)\n",
    "\n",
    "        # always trade\n",
    "        b = initial_bank\n",
    "        for r in sub[\"thu_tue\"]:\n",
    "            b *= r\n",
    "            if b >= upper_thresh or b <= lower_thresh:\n",
    "                break\n",
    "        ratio_always.append(model_bal/b if b!=0 else np.nan)\n",
    "\n",
    "        # random with same chattiness\n",
    "        ch_prob = len(sub.loc[sub[\"Outcome\"].isin([\"TP\",\"FP\"])])/len(sub)\n",
    "        b = initial_bank\n",
    "        for r in sub[\"thu_tue\"]:\n",
    "            if rng.random()<ch_prob:\n",
    "                b*=r\n",
    "        ratio_random.append(model_bal/b if b!=0 else np.nan)\n",
    "\n",
    "        # alternate week\n",
    "        b = initial_bank\n",
    "        for i,r in enumerate(sub[\"thu_tue\"]):\n",
    "            if i%2==0:\n",
    "                b*=r\n",
    "        ratio_alt.append(model_bal/b if b!=0 else np.nan)\n",
    "\n",
    "        # weighted coin\n",
    "        good_rate = float((sub[\"thu_tue\"]>1).mean())\n",
    "        b = initial_bank\n",
    "        for r in sub[\"thu_tue\"]:\n",
    "            if rng.random()<good_rate:\n",
    "                b*=r\n",
    "        ratio_weighted.append(model_bal/b if b!=0 else np.nan)\n",
    "\n",
    "    # ============================================================\n",
    "    # ------------------------ FINAL REPORT ----------------------\n",
    "    # ============================================================\n",
    "\n",
    "    report = {\n",
    "        \"historical_mc\": {\n",
    "            \"simulated_mean\": simulated_mean,\n",
    "            \"simulated_median\": simulated_median,\n",
    "            \"ks_distance\": float(ks_d),\n",
    "            \"ks_p_value\": float(ks_p),\n",
    "            \"average_null_percentile\": avg_null,\n",
    "        },\n",
    "        \"future_mc\": {\n",
    "            \"future_mean\": future_mean,\n",
    "            \"future_median\": future_median,\n",
    "            \"prob_above_initial\": prob_above_initial,\n",
    "            \"prob_success\": prob_success,\n",
    "            \"prob_failure\": prob_failure,\n",
    "            \"prob_uncertain\": prob_uncertain,\n",
    "        },\n",
    "        \"internal_metrics\": {\n",
    "            \"precision_overall\": prec_overall,\n",
    "            \"chattiness_overall\": chat_overall,\n",
    "            \"correctness_rate\": correctness_rate,\n",
    "            \"precision_on_trades\": precision_on_trades,\n",
    "            \"trade_frequency\": trade_frequency,\n",
    "            \"mistake_asymmetry_%\": mistake_asymmetry,\n",
    "            \"macro_return_ratio\": macro,\n",
    "            \"micro_return_ratio\": micro,\n",
    "            \"return_ratio_gap\": gap,\n",
    "            \"longest_TP_streak\": longest_tp,\n",
    "            \"longest_FP_streak\": longest_fp,\n",
    "            \"%FP_when_predicted_positive\": pct_fp_positive,\n",
    "        },\n",
    "        \"baseline_comparison\": {\n",
    "            \"vs_always_trade\": float(np.nanmean(ratio_always)),\n",
    "            \"vs_random_trader\": float(np.nanmean(ratio_random)),\n",
    "            \"vs_alternate_trader\": float(np.nanmean(ratio_alt)),\n",
    "            \"vs_weighted_coin\": float(np.nanmean(ratio_weighted)),\n",
    "        },\n",
    "        \"uniformity_test\": uniformity_test,\n",
    "        \"randomness_test\": randomness_test,\n",
    "    }\n",
    "\n",
    "    pprint(report)\n",
    "    return report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "souq_e_commerce_3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
